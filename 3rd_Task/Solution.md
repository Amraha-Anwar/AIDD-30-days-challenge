# Solution  
**by AmrahağŸ˜**

## ğŸŠ Part A  

1. **Whatâ€™s new in Gemini 3.0?**  
   - It reasons *much* smarter now, especially for hard, multi-step problems.   
   - You can control how â€œdeepâ€ it thinks using a `thinking_level` setting, pick speed or depth as you like.   
   - It can handle *huge context* up to **1 million tokens**, so itâ€™s great for big docs, long code, or complex workflows.    


2. **How does Gemini 3.0 improve coding & automation?**  
   - Itâ€™s built for agentic coding, Gemini agents can actually plan, write, test, and run code by themselves.  
   - Through **Google Antigravity**, multiple agents can work in an editor, terminal, and browser and they leave behind â€œartifactsâ€ (plans, screenshots) so you know what they did.   
   - Thereâ€™s a **CLI tool** (Gemini CLI) so you can use its reasoning and coding power right from your terminal.   


3. **How does Gemini 3.0 improve multimodal understanding?**  
   - It combines text, images, video, audio, and even code all in one model, letting it understand and reason across them.    
   - Itâ€™s strong in **visual reasoning** not just reading text in images but understanding layouts, structure, and context.   
   - It also has **spatial reasoning**, making it good at tasks involving screen UI, trajectories, pointing, or other spatial contexts.   


4. **Two developer tools in Gemini 3.0**  
   - **Google Antigravity**: A brand new IDE built around agent first workflows.   
   - **Gemini CLI**: Use Geminiâ€™s agentic and reasoning power from your terminal, super useful for coding and automation. (Which we are currently using for our projectsğŸ˜)


---


## ğŸŠ Part B

***GEMINI CLI MODEL UPDATION***


![cli_ss](CLI.png)


---